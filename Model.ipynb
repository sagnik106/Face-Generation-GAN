{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Flatten, Reshape, Activation, Dense, UpSampling2D, LeakyReLU, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=list()\n",
    "for i in range(1,1000):\n",
    "    a=cv2.imread(\"img_align_celeba/%06d.jpg\"%i,1)\n",
    "    a=cv2.resize(a,(a.shape[0]//2-1,a.shape[1]//2-1))\n",
    "    a=a/255.0\n",
    "    a=a.tolist()\n",
    "    images.append(a)\n",
    "images=np.asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(inputs, image_shape):\n",
    "    \n",
    "    x=Dense((image_shape[0]//4) * (image_shape[1]//4) * 128)(inputs)\n",
    "    x=Reshape((image_shape[0]//4, image_shape[1]//4, 128))(x)\n",
    "    \n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Conv2DTranspose(filters = 128, kernel_size=5, strides=2, padding='same', activation = 'sigmoid')(x)\n",
    "    \n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Conv2DTranspose(filters = 64, kernel_size=5, strides=2, padding='same', activation = 'sigmoid')(x)\n",
    "    \n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Conv2DTranspose(filters = 32, kernel_size=5, strides=1, padding='same', activation = 'sigmoid')(x)\n",
    "    \n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Conv2DTranspose(filters = 3, kernel_size=5, strides=1, padding='same', activation = 'sigmoid')(x)\n",
    "        \n",
    "    generator=Model(inputs, x)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis(inputs):\n",
    "    x=inputs\n",
    "    \n",
    "    x=LeakyReLU(alpha=0.2)(x)\n",
    "    x= Conv2D(filters= 32, kernel_size=5, strides=2, padding='same')(x)\n",
    "    \n",
    "    x=LeakyReLU(alpha=0.2)(x)\n",
    "    x= Conv2D(filters= 32, kernel_size=5, strides=2, padding='same')(x)\n",
    "    \n",
    "    x=LeakyReLU(alpha=0.2)(x)\n",
    "    x= Conv2D(filters= 32, kernel_size=5, strides=2, padding='same')(x)\n",
    "    \n",
    "    x=LeakyReLU(alpha=0.2)(x)\n",
    "    x= Conv2D(filters= 32, kernel_size=5, strides=1, padding='same')(x)\n",
    "    \n",
    "    x=Flatten()(x)\n",
    "    x=Dense(1, activation = 'sigmoid')(x)\n",
    "    \n",
    "    discriminator = Model(inputs, x)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(latent_size):\n",
    "    input_shape=(images.shape[1],images.shape[2],images.shape[3])\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    discriminator=dis(inputs)\n",
    "    discriminator.compile(loss = 'binary_crossentropy', optimizer=RMSprop(lr= 2e-4, decay=6e-8), metrics = ['accuracy'])\n",
    "    \n",
    "    inputs = Input(shape=(latent_size, ))\n",
    "    generator=gen(inputs, input_shape)\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    adversarial=Model(inputs, discriminator(generator(inputs)))\n",
    "    adversarial.compile(loss='binary_crossentropy', optimizer = RMSprop(lr=2e-4*0.5,decay=6e-8*0.5))\n",
    "    \n",
    "    models=(generator, discriminator, adversarial)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, epochs, batch_size, latent_size, save_interval=0):\n",
    "    generator, discriminator, adversarial = models\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        rand_indices = np.random.randint(0, images.shape[0], size = batch_size)\n",
    "        real = images[rand_indices]\n",
    "        noise = np.random.uniform(0, 1.0, size = [batch_size, latent_size])\n",
    "        fake = generator.predict(noise)\n",
    "        \n",
    "        x = np.concatenate((real,fake))\n",
    "        \n",
    "        y = np.ones(batch_size*2)\n",
    "        y[batch_size:]=0.0\n",
    "        y=np.reshape(y,(batch_size*2,1))\n",
    "        \n",
    "        history = discriminator.train_on_batch(x,y)\n",
    "        \n",
    "        noise = np.random.uniform(0, 1.0, size = [batch_size, latent_size])\n",
    "        y=np.ones(batch_size)\n",
    "        y=np.reshape(y,(batch_size,1))\n",
    "        \n",
    "        \"\"\"aloss, aacc =\"\"\" \n",
    "        history1 = adversarial.train_on_batch(noise, y)\n",
    "        \n",
    "        print(\"Epoch : %d, [adversarial loss: %f] [discriminator loss: %f, acc: %f]\"%(i+1, history1, history[0], history[1]))\n",
    "        \n",
    "        if save_interval!=0:\n",
    "            if i%save_interval==0:\n",
    "                noise = np.random.uniform(0, 1.0, size = [1, latent_size])\n",
    "                n=generator.predict(noise)\n",
    "                cv2.imwrite(\"%d.jpeg\"%i,n[0])\n",
    "                print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sagnik106/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/sagnik106/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch : 1, [adversarial loss: 0.686946] [discriminator loss: 0.696691, acc: 0.546875]\n",
      "Saved!\n",
      "Epoch : 2, [adversarial loss: 0.154052] [discriminator loss: 0.657645, acc: 0.500000]\n",
      "Epoch : 3, [adversarial loss: 0.051610] [discriminator loss: 0.621702, acc: 0.992188]\n",
      "Epoch : 4, [adversarial loss: 0.020561] [discriminator loss: 0.574115, acc: 1.000000]\n",
      "Epoch : 5, [adversarial loss: 0.008457] [discriminator loss: 0.507111, acc: 0.992188]\n",
      "Epoch : 6, [adversarial loss: 0.060770] [discriminator loss: 0.449562, acc: 1.000000]\n",
      "Epoch : 7, [adversarial loss: 0.002496] [discriminator loss: 0.911287, acc: 0.523438]\n",
      "Epoch : 8, [adversarial loss: 0.012464] [discriminator loss: 0.626560, acc: 0.500000]\n",
      "Epoch : 9, [adversarial loss: 0.004576] [discriminator loss: 0.545981, acc: 0.757812]\n",
      "Epoch : 10, [adversarial loss: 0.003249] [discriminator loss: 0.480851, acc: 0.960938]\n",
      "Epoch : 11, [adversarial loss: 0.002295] [discriminator loss: 0.446405, acc: 0.976562]\n",
      "Saved!\n",
      "Epoch : 12, [adversarial loss: 0.001122] [discriminator loss: 0.454785, acc: 0.914062]\n",
      "Epoch : 13, [adversarial loss: 0.001954] [discriminator loss: 0.401999, acc: 1.000000]\n",
      "Epoch : 14, [adversarial loss: 0.000359] [discriminator loss: 0.436905, acc: 0.882812]\n",
      "Epoch : 15, [adversarial loss: 0.001814] [discriminator loss: 0.436443, acc: 0.500000]\n",
      "Epoch : 16, [adversarial loss: 0.000312] [discriminator loss: 0.483683, acc: 0.757812]\n",
      "Epoch : 17, [adversarial loss: 0.000964] [discriminator loss: 0.398934, acc: 1.000000]\n",
      "Epoch : 18, [adversarial loss: 0.000197] [discriminator loss: 0.427775, acc: 0.851562]\n",
      "Epoch : 19, [adversarial loss: 0.000460] [discriminator loss: 0.348989, acc: 0.992188]\n",
      "Epoch : 20, [adversarial loss: 0.000090] [discriminator loss: 0.315633, acc: 0.906250]\n"
     ]
    }
   ],
   "source": [
    "model=models(100)\n",
    "train(model, 1000, 64, 100, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
